{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping  - Mission to Mars\n",
    "\n",
    "![mission_to_mars](Images/mission_to_mars.png)\n",
    "\n",
    "A web application was built that scrapes various Mars related websites for data related to the Mission to Mars and displays the information in a single HTML page. The code to do the scraping is captured in a [Jupyter Notebook](Mission_to_Mars/mission_to_mars.ipynb). This code was revised and put into a python .py file named [scrape_mars.py](Mission_to_Mars/scrape_mars.py) in a function to be called upon by a flask [app.py](Mission_to_Mars/app.py) when a button is clicked on the website calling the /scrape path.\n",
    "\n",
    "\n",
    "##  Scraping Mars data\n",
    "\n",
    "Initial scraping was completed using [Jupyter Notebook](Mission_to_Mars/mission_to_mars.ipynb) using BeautifulSoup, Pandas, and Requests/Splinter. The locations and types of data that were scraped follows.\n",
    "\n",
    "\n",
    "### NASA Mars News\n",
    "\n",
    "* The [NASA Mars News Site](https://mars.nasa.gov/news/)  latest News Title and Paragraph Text was scraped. The data was assigned to the following variables for reference later:\n",
    "\n",
    "`news_title`\n",
    "\n",
    "`news_paragraph` \n",
    "\n",
    "### JPL Mars Space Images - Featured Image\n",
    "\n",
    "The JPL Featured Space Image was scraped from: [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars) and stored in the following variable:\n",
    "\n",
    "`featured_image_url`\n",
    "\n",
    "\n",
    "### Mars Weather\n",
    "\n",
    "The Mars Weather twitter account [here](https://twitter.com/marswxreport?lang=en) was scraped for the latest Mars weather tweet from the page. Tweets which were not weather tweets were filtered out in order to get the latest weather tweet. The result was stored in the following variable:\n",
    "\n",
    "`weather_data`\n",
    "\n",
    "\n",
    "### Mars Facts\n",
    "\n",
    "The Mars Facts webpage [here](https://space-facts.com/mars/) was used along with Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc. The information was stored in the following variable:\n",
    "\n",
    "`fact_table`\n",
    "\n",
    "### Mars Hemispheres\n",
    "\n",
    "The USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) was used to obtain four high resolution images for each of Mar's hemispheres. The urls and titles were stored in a variable as a dictionary similar to the example below:\n",
    "\n",
    "```python\n",
    "# Example:\n",
    "hemisphere_image_urls = [\n",
    "    {\"title\": \"Valles Marineris Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Cerberus Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Schiaparelli Hemisphere\", \"img_url\": \"...\"},\n",
    "    {\"title\": \"Syrtis Major Hemisphere\", \"img_url\": \"...\"},\n",
    "]\n",
    "```\n",
    "\n",
    "- - -\n",
    "\n",
    "##  MongoDB and Flask Application\n",
    "\n",
    "MongoDB with Flask templating was used to create a new HTML page that displays all of the information that was scraped from the URLs above.\n",
    "\n",
    "* The Jupyter notebook was transformed into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of the scraping code from above and return one Python dictionary containing all of the scraped data.\n",
    "\n",
    "* A route called `/scrape` was created that will import your `scrape_mars.py` script and call your `scrape` function.\n",
    "\n",
    "  * The return value was stored in Mongo as a Python dictionary.\n",
    "\n",
    "* A root route `/` was created that will query ther Mongo database and pass the mars data into an HTML template to display the data.\n",
    "\n",
    "* A template HTML file called `index.html` was created that takes the mars data dictionary and displays all of the data in the appropriate HTML elements. \n",
    "\n",
    "The results that are rendered after clicking the \"Scrape New Data\" button is below:\n",
    "\n",
    "![final_app_part1.png](Images/screencapture.png)\n",
    "\n",
    "\n",
    "- - -\n",
    "\n",
    "## Step 3 - Submission\n",
    "\n",
    "To submit your work to BootCampSpot, create a new GitHub repository and upload the following:\n",
    "\n",
    "1. The Jupyter Notebook containing the scraping code used.\n",
    "\n",
    "2. Screenshots of your final application.\n",
    "\n",
    "3. Submit the link to your new repository to BootCampSpot.\n",
    "\n",
    "## Hints\n",
    "\n",
    "* Use Splinter to navigate the sites when needed and BeautifulSoup to help find and parse out the necessary data.\n",
    "\n",
    "* Use Pymongo for CRUD applications for your database. For this homework, you can simply overwrite the existing document each time the `/scrape` url is visited and new data is obtained.\n",
    "\n",
    "* Use Bootstrap to structure your HTML template.\n",
    "\n",
    "### Copyright\n",
    "\n",
    "Trilogy Education Services Â© 2019. All Rights Reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
